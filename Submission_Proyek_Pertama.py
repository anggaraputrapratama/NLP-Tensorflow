# -*- coding: utf-8 -*-
"""Submission_Proyek Pertama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IbMwmPFYnXnmDb5zRT5l5QZlokIyy2-k

## [Anggara Putra Pratama](https://www.dicoding.com/users/anggaraputrapratama)

Proyek Pertama : Membuat Model NLP dengan TensorFlow

Dataset: https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment

# Kaggle Dataset
saya menggunakan kaggle API
"""

! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d crowdflower/twitter-airline-sentiment

! unzip twitter-airline-sentiment.zip

"""# Import Library"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# Load Dataset"""

df = pd.read_csv('Tweets.csv')
df.head()

df.info()

df_1 = df[['text', 'airline_sentiment']]
df_1

"""# Visualize Dataset
Visualize airline_sentiment
"""

sns.set_style("whitegrid")
sns.set(rc={'figure.figsize':(11,4)})
sns.countplot(df_1['airline_sentiment'])

"""# Text Processing"""

import re

def clean(text):
  text = text.lower()
  text = re.sub('\[.*?\]', '', text) # remove square brackets
  text = re.sub(r'[^\w\s]','',text) # remove punctuation
  text = re.sub('\w*\d\w*', '', text) # remove words containing numbers
  text = re.sub('\n', '', text)
  return text

df_1['text'] =  df_1['text'].apply(clean)
df_1

df_1.rename(columns={"text": "clean_text"})

kategori = pd.get_dummies(df_1.airline_sentiment)
df_baru = pd.concat([df_1, kategori], axis = 1)
df_baru = df_baru.drop(columns='airline_sentiment')
df_baru

tweets = df_baru['text'].values
label = df_baru[['negative', 'neutral', 'positive']].values

from sklearn.model_selection import train_test_split
tweets_latih, tweets_test, label_latih, label_test = train_test_split(tweets, label, test_size=0.2, random_state=42)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, lower =True, split=' ')
tokenizer.fit_on_texts(tweets_latih) 
tokenizer.fit_on_texts(tweets_test)
 
sekuens_latih = tokenizer.texts_to_sequences(tweets_latih)
sekuens_test = tokenizer.texts_to_sequences(tweets_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(200),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

num_epochs = 10
history = model.fit(padded_latih, label_latih, batch_size = 8,epochs=num_epochs, 
                    validation_data=(padded_test, label_test), callbacks=[callback], verbose=2)

results = model.evaluate(padded_test, label_test)
print(results)

accuracy = history.history['accuracy']
loss = history.history['loss']
val_accuracy = history.history['val_accuracy']
val_loss = history.history['val_loss']

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(range(1, len(accuracy)+1), accuracy, 'b', label='training_accuracy')
plt.plot(range(1, len(val_accuracy)+1), val_accuracy, 'g', label='val_accuracy')
plt.title('Training Accuracy and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(range(1, len(loss)+1), loss, 'b', label='training_loss')
plt.plot(range(1, len(val_loss)+1), val_loss, 'g', label='val_loss')
plt.title('Training Loss and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

